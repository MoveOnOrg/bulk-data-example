# Bulk Data API Redshift Pipeline
A [Terraform](https://www.terraform.io/) plan for implementing a data ETL pipeline from [ControlShift](https://www.controlshiftlabs.com) to [Amazon Redshift.](https://aws.amazon.com/redshift/)

The output of this plan is a replica of all of the tables that underlie your ControlShift instance in Redshift which allows
for querying via SQL or other analysis. 

## Overview

The Terraform plan sets up resources in your AWS environment to process webhooks generated by the ControlShift Bulk Data API. 

The integration is based on the [aws-lambda-redshift-loader](https://github.com/awslabs/aws-lambda-redshift-loader) provided by
AWS but replaces the manual setup steps from their README with a Terraform plan. In addition the Terraform plan includes
resourced that are specific to accepting ControlShift Bulk Data API webhooks.

The resources created include:

- DynamoDB tables that store configuration information and logs each table load processed.
- Lambda functions that process incoming webhooks, store CSV files onto S3 and load those files into tables in Redshift.
- S3 buckets for storing incoming S3 CSVs and manifests of load activity.
- A Web API Gateway to connect AWS Lambdas to the web.
- IAM permissions to make everything work securely.


## Prerequisites

- Familiarity with Amazon Web Services, Redshift, and Terraform
- Use of [aws-vault](https://github.com/99designs/aws-vault) or a similar tool for using AWS secrets securely. 
- The `terraform` command line tool. [Download](https://www.terraform.io/downloads.html) 
